{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70a84e7-18aa-4b5e-8bc5-91424dc936cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f0ed1307910>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essential imports:\n",
    "import pandas as pd\n",
    "import lucene\n",
    "import os\n",
    "import re\n",
    "from java.io import File\n",
    "\n",
    "# Indexer imports:\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "import org.apache.lucene.document as document\n",
    "\n",
    "# Retriever imports:\n",
    "from org.apache.lucene.index import IndexReader\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.index import DirectoryReader\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "\n",
    "from java.io import File\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b288a7c7-6b72-41a0-bdcc-8827b3193681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTFIDFSimilarity(ClassicSimilarity):\n",
    "    \n",
    "#     print(\"Welcome\")\n",
    "#     # query_length = 0\n",
    "#     # def __init__(self, qlength):\n",
    "#     #     self.query_length = qlength\n",
    "\n",
    "#     def tf(freq):\n",
    "#         print(\"TF called\")\n",
    "#         unique_terms = FieldInvertState.getUniqueTermCount() \n",
    "#         doc_length = FieldInvertState.getLength()\n",
    "#         AvgTF = doc_length / unique_terms\n",
    "#         RITF = math.log2(1+freq) / math.log2(1+AvgTF)\n",
    "#         total_freq = CollectionStatistics.sumTotalTermFreq()\n",
    "#         num_docs = CollectionStatistics.maxDoc()\n",
    "#         ADL = total_freq / num_docs\n",
    "#         LRTF = freq * math.log2(1+(ADL/doc_length))\n",
    "#         BRITF = RITF / (1+RITF)\n",
    "#         BLRTF = LRTF / (1+LRTF)\n",
    "#         w = 2 / (1 + math.log2(a+query_length))\n",
    "#         TFF = ( w * BRITF ) + ( (1-w) * BLRTF )\n",
    "#         return TFF\n",
    "\n",
    "#     def idf(docFreq, docCount):\n",
    "        \n",
    "#         raw_idf = math.log((docCount + 1) / docFreq )\n",
    "#         CTF = TermStatistics.totalTermFreq()\n",
    "#         AEF = CTF / freq\n",
    "#         TDF = raw_idf * (AEF/(1+AEF))\n",
    "#         return TDF\n",
    "    \n",
    "#     def lengthNorm(numTerms):\n",
    "#         # Custom length normalization\n",
    "#         return 1.0\n",
    "#     def TFIDFScorer(boost, idf, normTable):\n",
    "#         self.idf = idf\n",
    "#         self.boost = 1\n",
    "#         self.queryWeight = idf.getValue().floatValue()\n",
    "#         self.normTable = 1\n",
    "#     def score(freq, norm):\n",
    "#         raw = tf(freq)*queryWeight\n",
    "#         return 2*raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e97581f-097d-4f1d-a3d9-6dfe7cb7c75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Path: index>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexPath = File(\"index/\").toPath()\n",
    "indexPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1fcfc82-f0e5-49f6-92ed-2dc9f8d850f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38352\n",
      "527979\n",
      "504.6469423973302\n"
     ]
    }
   ],
   "source": [
    "from java.nio.file import Paths\n",
    "from org.apache.lucene.index import DirectoryReader, Term\n",
    "\n",
    "index_directory = FSDirectory.open(Paths.get(\"index/\"))\n",
    "reader = DirectoryReader.open(index_directory)\n",
    "\n",
    "# 1. Term Frequency (term, doc)\n",
    "term = Term('TEXT',\"example\")\n",
    "doc_id = 0  # Example document ID\n",
    "field = \"TEXT\"\n",
    "\n",
    "docfreq = reader.docFreq(term)\n",
    "print(docfreq)\n",
    "numDoc = reader.numDocs()\n",
    "print(numDoc)\n",
    "num_terms = 0\n",
    "for i in range(numDoc):\n",
    "    term_vector = reader.getTermVector(i,field)\n",
    "    num_terms += term_vector.getSumTotalTermFreq()\n",
    "ADL = num_terms / numDoc\n",
    "print(ADL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb556d4-939a-4c3a-a7ff-37c4ad99f8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.util import BytesRef, BytesRefIterator\n",
    "from org.apache.lucene.index import DirectoryReader, Term, TermsEnum\n",
    "analyzer = StandardAnalyzer()\n",
    "directory = FSDirectory.open(indexPath)\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "\n",
    "def scored_docs(query,numDoc):\n",
    "\n",
    "    terms = query.split()\n",
    "    query_length = len(terms)\n",
    "    w = 2 / (1 + math.log2(1 + query_length))\n",
    "    \n",
    "    field = \"TEXT\"\n",
    "        \n",
    "    document_dict = {}\n",
    "    query = QueryParser(field, analyzer).parse(query)\n",
    "    \n",
    "    scoreDocs = searcher.search(query, numDoc).scoreDocs\n",
    "    \n",
    "    for i in range(len(scoreDocs)):\n",
    "\n",
    "        doc_id = scoreDocs[i].doc\n",
    "        doc = searcher.doc(scoreDocs[i].doc)\n",
    "        \n",
    "        term_vector = reader.getTermVector(doc_id,field)\n",
    "        termsEnumvar = term_vector.iterator()\n",
    "        termsref = BytesRefIterator.cast_(termsEnumvar)\n",
    "        term_dict = {}\n",
    "        \n",
    "        while (termsref.next()):\n",
    "            \n",
    "            termval = TermsEnum.cast_(termsref)\n",
    "            term_doc = termval.term().utf8ToString()\n",
    "            tf_doc = termsEnumvar.totalTermFreq() \n",
    "            term_dict[term_doc] = tf_doc\n",
    "            \n",
    "        # print(term_dict)   \n",
    "        len_D = sum(term_dict.values())\n",
    "        num_terms = len(term_dict)\n",
    "        average_tf = len_D / num_terms\n",
    "\n",
    "        sum_TDF = 0\n",
    "        sim = 0\n",
    "\n",
    "        for term in terms :\n",
    "            if term in term_dict:\n",
    "                tf = term_dict[term]\n",
    "            else:\n",
    "                tf = 0\n",
    "            RITF = math.log2(1+tf) / math.log2(1+average_tf) \n",
    "            LRTF = tf * math.log2(1 + (ADL/len_D))\n",
    "            BRITF = RITF / (1 + RITF)\n",
    "            BLRTF = LRTF / (1 + LRTF)\n",
    "            \n",
    "            TFF = (w * BRITF) + ((1 - w) * BLRTF)\n",
    "        \n",
    "            docfreq = reader.docFreq(Term('TEXT',term))\n",
    "            numDoc = reader.numDocs()\n",
    "\n",
    "            if docfreq != 0:\n",
    "                IDF = math.log2((numDoc+1)/docfreq)\n",
    "                CTF = reader.totalTermFreq(Term(\"TEXT\", term))\n",
    "                AEF =  CTF/ docfreq\n",
    "                TDF = IDF * (AEF / (1 + AEF))\n",
    "            else :\n",
    "                TDF = 0\n",
    "            sum_TDF += TDF\n",
    "            sim += TFF * TDF\n",
    "    \n",
    "        document_dict[doc.get(\"DOC_NO\")] = sim / sum_TDF\n",
    "        term_dict.clear()\n",
    "    return document_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff3899e-3f4f-4c14-b15e-f504c8fa252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(document_dict,query_no):\n",
    "    if query_no <= 450 :\n",
    "        with open(\"trec678.txt\", \"a\") as file:\n",
    "            i = 1\n",
    "            for key, value in document_dict.items():\n",
    "                file.write(\"%d\\t Q0\\t %s\\t %d\\t %f\\t CS2320\\n\"%(query_no, key, i, value))\n",
    "                i += 1\n",
    "                if (i>1000):\n",
    "                    break\n",
    "    else :\n",
    "        with open(\"robust.txt\", \"a\") as file:\n",
    "            i = 1\n",
    "            for key, value in document_dict.items():\n",
    "                file.write(\"%d\\t Q0\\t %s\\t %d\\t %f\\t CS2320\\n\"%(query_no, key, i, value))\n",
    "                i += 1\n",
    "                if (i>1000):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752abfba-3b0a-4dc1-bb0c-a66ca13559fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1th file - trec678.xml\n",
      "Processing file 2th file - robust.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "stop_words = {'a','about','above','after','again','against','ain','all','am','an','and','any','are','aren',\"aren't\",'as','at','be','because','been','before','being','below','between','both','but','by','can','couldn',\"couldn't\",'d','did','didn',\"didn't\",'do','does','doesn',\"doesn't\",'doing','don',\"don't\",'down','during','each','few','for','from','further','had','hadn',\"hadn't\",'has','hasn',\"hasn't\",'have','haven',\"haven't\",'having','he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','isn',\"isn't\",'it',\"it's\",'its','itself','just','ll','m','ma','me','mightn',\"mightn't\",'more','most','mustn',\"mustn't\",'my','myself','needn',\"needn't\",'no','nor','not','now','o','of','off','on','once','only','or','other','our','ours','ourselves','out','over','own','re','s','same','shan',\"shan't\",'she',\"she's\",'should',\"should've\",'shouldn',\"shouldn't\",'so','some','such','t','than','that',\"that'll\",'the','their','theirs','them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','ve','very','was','wasn',\"wasn't\",'we','were','weren',\"weren't\",'what','when','where','which','while','who','whom','why','will','with','won',\"won't\",'wouldn',\"wouldn't\",'y','you',\"you'd\",\"you'll\",\"you're\",\"you've\",'your','yours','yourself','yourselves'}\n",
    "def query_parser(file_path):\n",
    "\n",
    "    i = 1\n",
    "    list_query = []\n",
    "    \n",
    "    directory = os.fsencode(file_path)\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        f = open(file_path+filename,encoding=\"latin\")\n",
    "        line = f.readline()\n",
    "        \n",
    "        print(\"Processing file %dth file -\"%i,filename)\n",
    "        i += 1\n",
    "        while line :\n",
    "            if \"<num>\" in line :\n",
    "                num = re.sub('<[/]*\\w+>', '', line).strip()\n",
    "            if \"<title>\" in line :\n",
    "                query = re.sub('<[/]*\\w+>', '', line).lower().strip()\n",
    "                input_string = query.lower()\n",
    "                special_chars_pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "                query = re.sub(special_chars_pattern, ' ', input_string).split()\n",
    "                filtered_sentence = [word for word in query if word.lower() not in stop_words]\n",
    "                query = ' '.join(filtered_sentence)\n",
    "                list_query.append((num,query))\n",
    "            line = f.readline()\n",
    "    return list_query\n",
    "\n",
    "list_query = query_parser(\"trec678rb/topics/\")\n",
    "len(list_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "848cba1b-c506-4c0b-a3fb-d792e02b8f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('301', 'international organized crime'),\n",
       " ('302', 'poliomyelitis post polio'),\n",
       " ('303', 'hubble telescope achievements'),\n",
       " ('304', 'endangered species mammals'),\n",
       " ('305', 'dangerous vehicles'),\n",
       " ('306', 'african civilian deaths'),\n",
       " ('307', 'new hydroelectric projects'),\n",
       " ('308', 'implant dentistry'),\n",
       " ('309', 'rap crime'),\n",
       " ('310', 'radio waves brain cancer'),\n",
       " ('311', 'industrial espionage'),\n",
       " ('312', 'hydroponics'),\n",
       " ('313', 'magnetic levitation maglev'),\n",
       " ('314', 'marine vegetation'),\n",
       " ('315', 'unexplained highway accidents'),\n",
       " ('316', 'polygamy polyandry polygyny'),\n",
       " ('317', 'unsolicited faxes'),\n",
       " ('318', 'best retirement country'),\n",
       " ('319', 'new fuel sources'),\n",
       " ('320', 'undersea fiber optic cable'),\n",
       " ('321', 'women parliaments'),\n",
       " ('322', 'international art crime'),\n",
       " ('323', 'literary journalistic plagiarism'),\n",
       " ('324', 'argentine british relations'),\n",
       " ('325', 'cult lifestyles'),\n",
       " ('326', 'ferry sinkings'),\n",
       " ('327', 'modern slavery'),\n",
       " ('328', 'pope beatifications'),\n",
       " ('329', 'mexican air pollution'),\n",
       " ('330', 'iran iraq cooperation'),\n",
       " ('331', 'world bank criticism'),\n",
       " ('332', 'income tax evasion'),\n",
       " ('333', 'antibiotics bacteria disease'),\n",
       " ('334', 'export controls cryptography'),\n",
       " ('335', 'adoptive biological parents'),\n",
       " ('336', 'black bear attacks'),\n",
       " ('337', 'viral hepatitis'),\n",
       " ('338', 'risk aspirin'),\n",
       " ('339', 'alzheimer drug treatment'),\n",
       " ('340', 'land mine ban'),\n",
       " ('341', 'airport security'),\n",
       " ('342', 'diplomatic expulsion'),\n",
       " ('343', 'police deaths'),\n",
       " ('344', 'abuses e mail'),\n",
       " ('345', 'overseas tobacco sales'),\n",
       " ('346', 'educational standards'),\n",
       " ('347', 'wildlife extinction'),\n",
       " ('348', 'agoraphobia'),\n",
       " ('349', 'metabolism'),\n",
       " ('350', 'health computer terminals'),\n",
       " ('351', 'falkland petroleum exploration'),\n",
       " ('352', 'british chunnel impact'),\n",
       " ('353', 'antarctica exploration'),\n",
       " ('354', 'journalist risks'),\n",
       " ('355', 'ocean remote sensing'),\n",
       " ('356', 'postmenopausal estrogen britain'),\n",
       " ('357', 'territorial waters dispute'),\n",
       " ('358', 'blood alcohol fatalities'),\n",
       " ('359', 'mutual fund predictors'),\n",
       " ('360', 'drug legalization benefits'),\n",
       " ('361', 'clothing sweatshops'),\n",
       " ('362', 'human smuggling'),\n",
       " ('363', 'transportation tunnel disasters'),\n",
       " ('364', 'rabies'),\n",
       " ('365', 'el nino'),\n",
       " ('366', 'commercial cyanide uses'),\n",
       " ('367', 'piracy'),\n",
       " ('368', 'vitro fertilization'),\n",
       " ('369', 'anorexia nervosa bulimia'),\n",
       " ('370', 'food drug laws'),\n",
       " ('371', 'health insurance holistic'),\n",
       " ('372', 'native american casino'),\n",
       " ('373', 'encryption equipment export'),\n",
       " ('374', 'nobel prize winners'),\n",
       " ('375', 'hydrogen energy'),\n",
       " ('376', 'world court'),\n",
       " ('377', 'cigar smoking'),\n",
       " ('378', 'euro opposition'),\n",
       " ('379', 'mainstreaming'),\n",
       " ('380', 'obesity medical treatment'),\n",
       " ('381', 'alternative medicine'),\n",
       " ('382', 'hydrogen fuel automobiles'),\n",
       " ('383', 'mental illness drugs'),\n",
       " ('384', 'space station moon'),\n",
       " ('385', 'hybrid fuel cars'),\n",
       " ('386', 'teaching disabled children'),\n",
       " ('387', 'radioactive waste'),\n",
       " ('388', 'organic soil enhancement'),\n",
       " ('389', 'illegal technology transfer'),\n",
       " ('390', 'orphan drugs'),\n",
       " ('391', 'r amp drug prices'),\n",
       " ('392', 'robotics'),\n",
       " ('393', 'mercy killing'),\n",
       " ('394', 'home schooling'),\n",
       " ('395', 'tourism'),\n",
       " ('396', 'sick building syndrome'),\n",
       " ('397', 'automobile recalls'),\n",
       " ('398', 'dismantling europe arsenal'),\n",
       " ('399', 'oceanographic vessels'),\n",
       " ('400', 'amazon rain forest'),\n",
       " ('401', 'foreign minorities germany'),\n",
       " ('402', 'behavioral genetics'),\n",
       " ('403', 'osteoporosis'),\n",
       " ('404', 'ireland peace talks'),\n",
       " ('405', 'cosmic events'),\n",
       " ('406', 'parkinson disease'),\n",
       " ('407', 'poaching wildlife preserves'),\n",
       " ('408', 'tropical storms'),\n",
       " ('409', 'legal pan 103'),\n",
       " ('410', 'schengen agreement'),\n",
       " ('411', 'salvaging shipwreck treasure'),\n",
       " ('412', 'airport security'),\n",
       " ('413', 'steel production'),\n",
       " ('414', 'cuba sugar exports'),\n",
       " ('415', 'drugs golden triangle'),\n",
       " ('416', 'three gorges project'),\n",
       " ('417', 'creativity'),\n",
       " ('418', 'quilts income'),\n",
       " ('419', 'recycle automobile tires'),\n",
       " ('420', 'carbon monoxide poisoning'),\n",
       " ('421', 'industrial waste disposal'),\n",
       " ('422', 'art stolen forged'),\n",
       " ('423', 'milosevic mirjana markovic'),\n",
       " ('424', 'suicides'),\n",
       " ('425', 'counterfeiting money'),\n",
       " ('426', 'law enforcement dogs'),\n",
       " ('427', 'uv damage eyes'),\n",
       " ('428', 'declining birth rates'),\n",
       " ('429', 'legionnaires disease'),\n",
       " ('430', 'killer bee attacks'),\n",
       " ('431', 'robotic technology'),\n",
       " ('432', 'profiling motorists police'),\n",
       " ('433', 'greek philosophy stoicism'),\n",
       " ('434', 'estonia economy'),\n",
       " ('435', 'curbing population growth'),\n",
       " ('436', 'railway accidents'),\n",
       " ('437', 'deregulation gas electric'),\n",
       " ('438', 'tourism increase'),\n",
       " ('439', 'inventions scientific discoveries'),\n",
       " ('440', 'child labor'),\n",
       " ('441', 'lyme disease'),\n",
       " ('442', 'heroic acts'),\n",
       " ('443', 'u investment africa'),\n",
       " ('444', 'supercritical fluids'),\n",
       " ('445', 'women clergy'),\n",
       " ('446', 'tourists violence'),\n",
       " ('447', 'stirling engine'),\n",
       " ('448', 'ship losses'),\n",
       " ('449', 'antibiotics ineffectiveness'),\n",
       " ('450', 'king hussein peace'),\n",
       " ('601', 'turkey iraq water'),\n",
       " ('602', 'czech slovak sovereignty'),\n",
       " ('603', 'tobacco cigarette lawsuit'),\n",
       " ('604', 'lyme disease arthritis'),\n",
       " ('605', 'great britain health care'),\n",
       " ('606', 'leg traps ban'),\n",
       " ('607', 'human genetic code'),\n",
       " ('608', 'taxing social security'),\n",
       " ('609', 'per capita alcohol consumption'),\n",
       " ('610', 'minimum wage adverse impact'),\n",
       " ('611', 'kurds germany violence'),\n",
       " ('612', 'tibet protesters'),\n",
       " ('613', 'berlin wall disposal'),\n",
       " ('614', 'flavr savr tomato'),\n",
       " ('615', 'timber exports asia'),\n",
       " ('616', 'volkswagen mexico'),\n",
       " ('617', 'russia cuba economy'),\n",
       " ('618', 'ayatollah khomeini death'),\n",
       " ('619', 'winnie mandela scandal'),\n",
       " ('620', 'france nuclear testing'),\n",
       " ('621', 'women ordained church england'),\n",
       " ('622', 'price fixing'),\n",
       " ('623', 'toxic chemical weapon'),\n",
       " ('624', 'sdi star wars'),\n",
       " ('625', 'arrests bombing wtc'),\n",
       " ('626', 'human stampede'),\n",
       " ('627', 'russian food crisis'),\n",
       " ('628', 'u invasion panama'),\n",
       " ('629', 'abortion clinic attack'),\n",
       " ('630', 'gulf war syndrome'),\n",
       " ('631', 'mandela south africa president'),\n",
       " ('632', 'southeast asia tin mining'),\n",
       " ('633', 'welsh devolution'),\n",
       " ('634', 'l tryptophan deaths'),\n",
       " ('635', 'doctor assisted suicides'),\n",
       " ('636', 'jury duty exemptions'),\n",
       " ('637', 'human growth hormone hgh'),\n",
       " ('638', 'wrongful convictions'),\n",
       " ('639', 'consumer line shopping'),\n",
       " ('640', 'maternity leave policies'),\n",
       " ('641', 'valdez wildlife marine life'),\n",
       " ('642', 'tiananmen square protesters'),\n",
       " ('643', 'salmon dams pacific northwest'),\n",
       " ('644', 'exotic animals import'),\n",
       " ('645', 'software piracy'),\n",
       " ('646', 'food stamps increase'),\n",
       " ('647', 'windmill electricity'),\n",
       " ('648', 'family leave law'),\n",
       " ('649', 'computer viruses'),\n",
       " ('650', 'tax evasion indicted'),\n",
       " ('651', 'u ethnic population'),\n",
       " ('652', 'oic balkans 1990s'),\n",
       " ('653', 'eta basque terrorism'),\n",
       " ('654', 'sex schools'),\n",
       " ('655', 'add diagnosis treatment'),\n",
       " ('656', 'lead poisoning children'),\n",
       " ('657', 'school prayer banned'),\n",
       " ('658', 'teenage pregnancy'),\n",
       " ('659', 'cruise health safety'),\n",
       " ('660', 'whale watching california'),\n",
       " ('661', 'melanoma treatment causes'),\n",
       " ('662', 'telemarketer protection'),\n",
       " ('663', 'agent orange exposure'),\n",
       " ('664', 'american indian museum'),\n",
       " ('665', 'poverty africa sub sahara'),\n",
       " ('666', 'thatcher resignation impact'),\n",
       " ('667', 'unmarried partner households'),\n",
       " ('668', 'poverty disease'),\n",
       " ('669', 'islamic revolution'),\n",
       " ('670', 'u elections apathy'),\n",
       " ('671', 'salvation army benefits'),\n",
       " ('672', 'nra membership profile'),\n",
       " ('673', 'soviet withdrawal afghanistan'),\n",
       " ('674', 'greenpeace prosecuted'),\n",
       " ('675', 'olympics training swimming'),\n",
       " ('676', 'poppy cultivation'),\n",
       " ('677', 'leaning tower pisa'),\n",
       " ('678', 'joint custody impact'),\n",
       " ('679', 'opening adoption records'),\n",
       " ('680', 'immigrants spanish school'),\n",
       " ('681', 'wind power location'),\n",
       " ('682', 'adult immigrants english'),\n",
       " ('683', 'czechoslovakia breakup'),\n",
       " ('684', 'part time benefits'),\n",
       " ('685', 'oscar winner selection'),\n",
       " ('686', 'argentina pegging dollar'),\n",
       " ('687', 'northern ireland industry'),\n",
       " ('688', 'non u media bias'),\n",
       " ('689', 'family planning aid'),\n",
       " ('690', 'college education advantage'),\n",
       " ('691', 'clear cutting forests'),\n",
       " ('692', 'prostate cancer detection treatment'),\n",
       " ('693', 'newspapers electronic media'),\n",
       " ('694', 'compost pile'),\n",
       " ('695', 'white collar crime sentence'),\n",
       " ('696', 'safety plastic surgery'),\n",
       " ('697', 'air traffic controller'),\n",
       " ('698', 'literacy rates africa'),\n",
       " ('699', 'term limits'),\n",
       " ('700', 'gasoline tax u')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36186c13-9db9-4d9d-a400-5d0f23b4d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processes query 301\n",
      "Processes query 302\n",
      "Processes query 303\n",
      "Processes query 304\n",
      "Processes query 305\n",
      "Processes query 306\n",
      "Processes query 307\n",
      "Processes query 308\n",
      "Processes query 309\n",
      "Processes query 310\n",
      "Processes query 311\n",
      "Processes query 312\n",
      "Processes query 313\n",
      "Processes query 314\n",
      "Processes query 315\n",
      "Processes query 316\n",
      "Processes query 317\n",
      "Processes query 318\n",
      "Processes query 319\n",
      "Processes query 320\n",
      "Processes query 321\n",
      "Processes query 322\n",
      "Processes query 323\n",
      "Processes query 324\n",
      "Processes query 325\n",
      "Processes query 326\n",
      "Processes query 327\n",
      "Processes query 328\n",
      "Processes query 329\n",
      "Processes query 330\n",
      "Processes query 331\n",
      "Processes query 332\n",
      "Processes query 333\n",
      "Processes query 334\n",
      "Processes query 335\n",
      "Processes query 336\n",
      "Processes query 337\n",
      "Processes query 338\n",
      "Processes query 339\n",
      "Processes query 340\n",
      "Processes query 341\n",
      "Processes query 342\n",
      "Processes query 343\n",
      "Processes query 344\n",
      "Processes query 345\n",
      "Processes query 346\n",
      "Processes query 347\n",
      "Processes query 348\n",
      "Processes query 349\n",
      "Processes query 350\n",
      "Processes query 351\n",
      "Processes query 352\n",
      "Processes query 353\n",
      "Processes query 354\n",
      "Processes query 355\n",
      "Processes query 356\n",
      "Processes query 357\n",
      "Processes query 358\n",
      "Processes query 359\n",
      "Processes query 360\n",
      "Processes query 361\n",
      "Processes query 362\n",
      "Processes query 363\n",
      "Processes query 364\n",
      "Processes query 365\n",
      "Processes query 366\n",
      "Processes query 367\n",
      "Processes query 368\n",
      "Processes query 369\n",
      "Processes query 370\n",
      "Processes query 371\n",
      "Processes query 372\n",
      "Processes query 373\n",
      "Processes query 374\n",
      "Processes query 375\n",
      "Processes query 376\n",
      "Processes query 377\n",
      "Processes query 378\n",
      "Processes query 379\n",
      "Processes query 380\n",
      "Processes query 381\n",
      "Processes query 382\n",
      "Processes query 383\n",
      "Processes query 384\n",
      "Processes query 385\n",
      "Processes query 386\n",
      "Processes query 387\n",
      "Processes query 388\n",
      "Processes query 389\n",
      "Processes query 390\n",
      "Processes query 391\n",
      "Processes query 392\n",
      "Processes query 393\n",
      "Processes query 394\n",
      "Processes query 395\n",
      "Processes query 396\n",
      "Processes query 397\n",
      "Processes query 398\n",
      "Processes query 399\n",
      "Processes query 400\n",
      "Processes query 401\n",
      "Processes query 402\n",
      "Processes query 403\n",
      "Processes query 404\n",
      "Processes query 405\n",
      "Processes query 406\n",
      "Processes query 407\n",
      "Processes query 408\n",
      "Processes query 409\n",
      "Processes query 410\n",
      "Processes query 411\n",
      "Processes query 412\n",
      "Processes query 413\n",
      "Processes query 414\n",
      "Processes query 415\n",
      "Processes query 416\n",
      "Processes query 417\n",
      "Processes query 418\n",
      "Processes query 419\n",
      "Processes query 420\n",
      "Processes query 421\n",
      "Processes query 422\n",
      "Processes query 423\n",
      "Processes query 424\n",
      "Processes query 425\n",
      "Processes query 426\n",
      "Processes query 427\n",
      "Processes query 428\n",
      "Processes query 429\n",
      "Processes query 430\n",
      "Processes query 431\n",
      "Processes query 432\n",
      "Processes query 433\n",
      "Processes query 434\n",
      "Processes query 435\n",
      "Processes query 436\n",
      "Processes query 437\n",
      "Processes query 438\n",
      "Processes query 439\n",
      "Processes query 440\n",
      "Processes query 441\n",
      "Processes query 442\n",
      "Processes query 443\n",
      "Processes query 444\n",
      "Processes query 445\n",
      "Processes query 446\n",
      "Processes query 447\n",
      "Processes query 448\n",
      "Processes query 449\n",
      "Processes query 450\n",
      "Processes query 601\n",
      "Processes query 602\n",
      "Processes query 603\n",
      "Processes query 604\n",
      "Processes query 605\n",
      "Processes query 606\n",
      "Processes query 607\n",
      "Processes query 608\n",
      "Processes query 609\n",
      "Processes query 610\n",
      "Processes query 611\n",
      "Processes query 612\n",
      "Processes query 613\n",
      "Processes query 614\n",
      "Processes query 615\n",
      "Processes query 616\n",
      "Processes query 617\n",
      "Processes query 618\n",
      "Processes query 619\n",
      "Processes query 620\n",
      "Processes query 621\n",
      "Processes query 622\n",
      "Processes query 623\n",
      "Processes query 624\n",
      "Processes query 625\n",
      "Processes query 626\n",
      "Processes query 627\n",
      "Processes query 628\n",
      "Processes query 629\n",
      "Processes query 630\n",
      "Processes query 631\n",
      "Processes query 632\n",
      "Processes query 633\n",
      "Processes query 634\n",
      "Processes query 635\n",
      "Processes query 636\n",
      "Processes query 637\n",
      "Processes query 638\n",
      "Processes query 639\n",
      "Processes query 640\n",
      "Processes query 641\n",
      "Processes query 642\n",
      "Processes query 643\n",
      "Processes query 644\n",
      "Processes query 645\n",
      "Processes query 646\n",
      "Processes query 647\n",
      "Processes query 648\n",
      "Processes query 649\n",
      "Processes query 650\n",
      "Processes query 651\n",
      "Processes query 652\n",
      "Processes query 653\n",
      "Processes query 654\n",
      "Processes query 655\n",
      "Processes query 656\n",
      "Processes query 657\n",
      "Processes query 658\n",
      "Processes query 659\n",
      "Processes query 660\n",
      "Processes query 661\n",
      "Processes query 662\n",
      "Processes query 663\n",
      "Processes query 664\n",
      "Processes query 665\n",
      "Processes query 666\n",
      "Processes query 667\n",
      "Processes query 668\n",
      "Processes query 669\n",
      "Processes query 670\n",
      "Processes query 671\n",
      "Processes query 672\n",
      "Processes query 673\n",
      "Processes query 674\n",
      "Processes query 675\n",
      "Processes query 676\n",
      "Processes query 677\n",
      "Processes query 678\n",
      "Processes query 679\n",
      "Processes query 680\n",
      "Processes query 681\n",
      "Processes query 682\n",
      "Processes query 683\n",
      "Processes query 684\n",
      "Processes query 685\n",
      "Processes query 686\n",
      "Processes query 687\n",
      "Processes query 688\n",
      "Processes query 689\n",
      "Processes query 690\n",
      "Processes query 691\n",
      "Processes query 692\n",
      "Processes query 693\n",
      "Processes query 694\n",
      "Processes query 695\n",
      "Processes query 696\n",
      "Processes query 697\n",
      "Processes query 698\n",
      "Processes query 699\n",
      "Processes query 700\n"
     ]
    }
   ],
   "source": [
    "for query in list_query:\n",
    "    document_dict = scored_docs(query[1],numDoc)\n",
    "    sorted_document_dict = dict(sorted(document_dict.items(), key=lambda item: item[1],reverse=True))\n",
    "    write_file(sorted_document_dict,int(query[0]))\n",
    "    print(\"Processes query %d\"%int(query[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
